{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7a31440f-8854-4a8b-ad85-ab1f0636aaf5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fetching 50 files: 100%|██████████| 50/50 [00:00<00:00, 16180.48it/s]\n",
      "Loading checkpoint shards: 100%|██████████| 50/50 [01:01<00:00,  1.22s/it]\n",
      "Some parameters are on the meta device because they were offloaded to the cpu and disk.\n"
     ]
    }
   ],
   "source": [
    "import argparse\n",
    "import os\n",
    "import sys\n",
    "import subprocess\n",
    "from dataclasses import dataclass\n",
    "from typing import List, Optional, Dict, Any\n",
    "import json\n",
    "import re\n",
    "from datetime import datetime\n",
    "\n",
    "try:\n",
    "    from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
    "    import torch\n",
    "except ImportError:\n",
    "    print(\"Error: transformers and torch are required. Install with:\")\n",
    "    print(\"pip install transformers torch\")\n",
    "\n",
    "model_name=\"meta-llama/Llama-4-Scout-17B-16E\"\n",
    "#model_name='/apps/models/huggingface/hub/llama-4-scout-17b-8bit'\n",
    "#model_name='mradermacher/Llama-4-Scout-17B-16E-GGUF'\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "        model_name,\n",
    "        torch_dtype=torch.float16 if device.type == \"cuda\" else torch.float32,\n",
    "        device_map=\"auto\" if device.type == \"cuda\" else None\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e18a98a4-4bb4-4f7d-b250-31e93e82e944",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/apps/jupyter/.venv/lib/python3.13/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Loading checkpoint shards: 100%|██████████| 50/50 [00:46<00:00,  1.08it/s]\n",
      "Some parameters are on the meta device because they were offloaded to the disk and cpu.\n",
      "Device set to use cuda:1\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "expected mat1 and mat2 to have the same dtype, but got: c10::BFloat16 != c10::Half",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mRuntimeError\u001b[39m                              Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 16\u001b[39m\n\u001b[32m      4\u001b[39m pipe = pipeline(\u001b[33m\"\u001b[39m\u001b[33mimage-text-to-text\u001b[39m\u001b[33m\"\u001b[39m, model=\u001b[33m\"\u001b[39m\u001b[33mmeta-llama/Llama-4-Scout-17B-16E\u001b[39m\u001b[33m\"\u001b[39m, torch_dtype=torch.float16,\n\u001b[32m      5\u001b[39m     device_map=\u001b[33m\"\u001b[39m\u001b[33mauto\u001b[39m\u001b[33m\"\u001b[39m  \u001b[38;5;66;03m# This distributes across available GPUs\u001b[39;00m\n\u001b[32m      6\u001b[39m                )\n\u001b[32m      7\u001b[39m messages = [\n\u001b[32m      8\u001b[39m     {\n\u001b[32m      9\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mrole\u001b[39m\u001b[33m\"\u001b[39m: \u001b[33m\"\u001b[39m\u001b[33muser\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m     14\u001b[39m     },\n\u001b[32m     15\u001b[39m ]\n\u001b[32m---> \u001b[39m\u001b[32m16\u001b[39m \u001b[43mpipe\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/apps/jupyter/.venv/lib/python3.13/site-packages/transformers/pipelines/image_text_to_text.py:339\u001b[39m, in \u001b[36mImageTextToTextPipeline.__call__\u001b[39m\u001b[34m(self, images, text, **kwargs)\u001b[39m\n\u001b[32m    336\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m _is_chat(text):\n\u001b[32m    337\u001b[39m     \u001b[38;5;66;03m# We have one or more prompts in list-of-dicts format, so this is chat mode\u001b[39;00m\n\u001b[32m    338\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(text[\u001b[32m0\u001b[39m], \u001b[38;5;28mdict\u001b[39m):\n\u001b[32m--> \u001b[39m\u001b[32m339\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[34;43m__call__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mChat\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mimages\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    340\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    341\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m images \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/apps/jupyter/.venv/lib/python3.13/site-packages/transformers/pipelines/base.py:1458\u001b[39m, in \u001b[36mPipeline.__call__\u001b[39m\u001b[34m(self, inputs, num_workers, batch_size, *args, **kwargs)\u001b[39m\n\u001b[32m   1450\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mnext\u001b[39m(\n\u001b[32m   1451\u001b[39m         \u001b[38;5;28miter\u001b[39m(\n\u001b[32m   1452\u001b[39m             \u001b[38;5;28mself\u001b[39m.get_iterator(\n\u001b[32m   (...)\u001b[39m\u001b[32m   1455\u001b[39m         )\n\u001b[32m   1456\u001b[39m     )\n\u001b[32m   1457\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1458\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mrun_single\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpreprocess_params\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mforward_params\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpostprocess_params\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/apps/jupyter/.venv/lib/python3.13/site-packages/transformers/pipelines/base.py:1465\u001b[39m, in \u001b[36mPipeline.run_single\u001b[39m\u001b[34m(self, inputs, preprocess_params, forward_params, postprocess_params)\u001b[39m\n\u001b[32m   1463\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mrun_single\u001b[39m(\u001b[38;5;28mself\u001b[39m, inputs, preprocess_params, forward_params, postprocess_params):\n\u001b[32m   1464\u001b[39m     model_inputs = \u001b[38;5;28mself\u001b[39m.preprocess(inputs, **preprocess_params)\n\u001b[32m-> \u001b[39m\u001b[32m1465\u001b[39m     model_outputs = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mforward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_inputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mforward_params\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1466\u001b[39m     outputs = \u001b[38;5;28mself\u001b[39m.postprocess(model_outputs, **postprocess_params)\n\u001b[32m   1467\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m outputs\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/apps/jupyter/.venv/lib/python3.13/site-packages/transformers/pipelines/base.py:1365\u001b[39m, in \u001b[36mPipeline.forward\u001b[39m\u001b[34m(self, model_inputs, **forward_params)\u001b[39m\n\u001b[32m   1363\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m inference_context():\n\u001b[32m   1364\u001b[39m         model_inputs = \u001b[38;5;28mself\u001b[39m._ensure_tensor_on_device(model_inputs, device=\u001b[38;5;28mself\u001b[39m.device)\n\u001b[32m-> \u001b[39m\u001b[32m1365\u001b[39m         model_outputs = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_inputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mforward_params\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1366\u001b[39m         model_outputs = \u001b[38;5;28mself\u001b[39m._ensure_tensor_on_device(model_outputs, device=torch.device(\u001b[33m\"\u001b[39m\u001b[33mcpu\u001b[39m\u001b[33m\"\u001b[39m))\n\u001b[32m   1367\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/apps/jupyter/.venv/lib/python3.13/site-packages/transformers/pipelines/image_text_to_text.py:430\u001b[39m, in \u001b[36mImageTextToTextPipeline._forward\u001b[39m\u001b[34m(self, model_inputs, generate_kwargs)\u001b[39m\n\u001b[32m    427\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mgeneration_config\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m generate_kwargs:\n\u001b[32m    428\u001b[39m     generate_kwargs[\u001b[33m\"\u001b[39m\u001b[33mgeneration_config\u001b[39m\u001b[33m\"\u001b[39m] = \u001b[38;5;28mself\u001b[39m.generation_config\n\u001b[32m--> \u001b[39m\u001b[32m430\u001b[39m generated_sequence = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m.\u001b[49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mmodel_inputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mgenerate_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    432\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m {\u001b[33m\"\u001b[39m\u001b[33mgenerated_sequence\u001b[39m\u001b[33m\"\u001b[39m: generated_sequence, \u001b[33m\"\u001b[39m\u001b[33mprompt_text\u001b[39m\u001b[33m\"\u001b[39m: prompt_text, \u001b[33m\"\u001b[39m\u001b[33minput_ids\u001b[39m\u001b[33m\"\u001b[39m: input_ids}\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/apps/jupyter/.venv/lib/python3.13/site-packages/torch/utils/_contextlib.py:120\u001b[39m, in \u001b[36mcontext_decorator.<locals>.decorate_context\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    117\u001b[39m \u001b[38;5;129m@functools\u001b[39m.wraps(func)\n\u001b[32m    118\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mdecorate_context\u001b[39m(*args, **kwargs):\n\u001b[32m    119\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[32m--> \u001b[39m\u001b[32m120\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/apps/jupyter/.venv/lib/python3.13/site-packages/transformers/generation/utils.py:2633\u001b[39m, in \u001b[36mGenerationMixin.generate\u001b[39m\u001b[34m(self, inputs, generation_config, logits_processor, stopping_criteria, prefix_allowed_tokens_fn, synced_gpus, assistant_model, streamer, negative_prompt_ids, negative_prompt_attention_mask, use_model_defaults, custom_generate, **kwargs)\u001b[39m\n\u001b[32m   2625\u001b[39m     input_ids, model_kwargs = \u001b[38;5;28mself\u001b[39m._expand_inputs_for_generation(\n\u001b[32m   2626\u001b[39m         input_ids=input_ids,\n\u001b[32m   2627\u001b[39m         expand_size=generation_config.num_return_sequences,\n\u001b[32m   2628\u001b[39m         is_encoder_decoder=\u001b[38;5;28mself\u001b[39m.config.is_encoder_decoder,\n\u001b[32m   2629\u001b[39m         **model_kwargs,\n\u001b[32m   2630\u001b[39m     )\n\u001b[32m   2632\u001b[39m     \u001b[38;5;66;03m# 12. run sample (it degenerates to greedy search when `generation_config.do_sample=False`)\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m2633\u001b[39m     result = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_sample\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   2634\u001b[39m \u001b[43m        \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2635\u001b[39m \u001b[43m        \u001b[49m\u001b[43mlogits_processor\u001b[49m\u001b[43m=\u001b[49m\u001b[43mprepared_logits_processor\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2636\u001b[39m \u001b[43m        \u001b[49m\u001b[43mstopping_criteria\u001b[49m\u001b[43m=\u001b[49m\u001b[43mprepared_stopping_criteria\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2637\u001b[39m \u001b[43m        \u001b[49m\u001b[43mgeneration_config\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgeneration_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2638\u001b[39m \u001b[43m        \u001b[49m\u001b[43msynced_gpus\u001b[49m\u001b[43m=\u001b[49m\u001b[43msynced_gpus\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2639\u001b[39m \u001b[43m        \u001b[49m\u001b[43mstreamer\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstreamer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2640\u001b[39m \u001b[43m        \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mmodel_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2641\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2643\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m generation_mode \u001b[38;5;129;01min\u001b[39;00m (GenerationMode.BEAM_SAMPLE, GenerationMode.BEAM_SEARCH):\n\u001b[32m   2644\u001b[39m     \u001b[38;5;66;03m# 11. interleave input_ids with `num_beams` additional sequences per batch\u001b[39;00m\n\u001b[32m   2645\u001b[39m     input_ids, model_kwargs = \u001b[38;5;28mself\u001b[39m._expand_inputs_for_generation(\n\u001b[32m   2646\u001b[39m         input_ids=input_ids,\n\u001b[32m   2647\u001b[39m         expand_size=generation_config.num_beams,\n\u001b[32m   2648\u001b[39m         is_encoder_decoder=\u001b[38;5;28mself\u001b[39m.config.is_encoder_decoder,\n\u001b[32m   2649\u001b[39m         **model_kwargs,\n\u001b[32m   2650\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/apps/jupyter/.venv/lib/python3.13/site-packages/transformers/generation/utils.py:3614\u001b[39m, in \u001b[36mGenerationMixin._sample\u001b[39m\u001b[34m(self, input_ids, logits_processor, stopping_criteria, generation_config, synced_gpus, streamer, **model_kwargs)\u001b[39m\n\u001b[32m   3611\u001b[39m model_inputs.update({\u001b[33m\"\u001b[39m\u001b[33moutput_hidden_states\u001b[39m\u001b[33m\"\u001b[39m: output_hidden_states} \u001b[38;5;28;01mif\u001b[39;00m output_hidden_states \u001b[38;5;28;01melse\u001b[39;00m {})\n\u001b[32m   3613\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m is_prefill:\n\u001b[32m-> \u001b[39m\u001b[32m3614\u001b[39m     outputs = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mmodel_inputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[32m   3615\u001b[39m     is_prefill = \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[32m   3616\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/apps/jupyter/.venv/lib/python3.13/site-packages/torch/nn/modules/module.py:1773\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1771\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1772\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1773\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/apps/jupyter/.venv/lib/python3.13/site-packages/torch/nn/modules/module.py:1784\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1779\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1780\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1781\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1782\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1783\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1784\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1786\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1787\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/apps/jupyter/.venv/lib/python3.13/site-packages/accelerate/hooks.py:175\u001b[39m, in \u001b[36madd_hook_to_module.<locals>.new_forward\u001b[39m\u001b[34m(module, *args, **kwargs)\u001b[39m\n\u001b[32m    173\u001b[39m         output = module._old_forward(*args, **kwargs)\n\u001b[32m    174\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m175\u001b[39m     output = \u001b[43mmodule\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_old_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    176\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m module._hf_hook.post_forward(module, output)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/apps/jupyter/.venv/lib/python3.13/site-packages/transformers/models/llama4/modeling_llama4.py:1281\u001b[39m, in \u001b[36mLlama4ForConditionalGeneration.forward\u001b[39m\u001b[34m(self, input_ids, pixel_values, attention_mask, position_ids, past_key_values, inputs_embeds, vision_feature_layer, vision_feature_select_strategy, labels, use_cache, output_attentions, output_hidden_states, return_dict, cache_position, logits_to_keep, image_sizes, **kwargs)\u001b[39m\n\u001b[32m   1278\u001b[39m     inputs_embeds = \u001b[38;5;28mself\u001b[39m.get_input_embeddings()(input_ids)\n\u001b[32m   1280\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m pixel_values \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1281\u001b[39m     image_features = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mget_image_features\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1282\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpixel_values\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpixel_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1283\u001b[39m \u001b[43m        \u001b[49m\u001b[43mvision_feature_layer\u001b[49m\u001b[43m=\u001b[49m\u001b[43mvision_feature_layer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1284\u001b[39m \u001b[43m        \u001b[49m\u001b[43mvision_feature_select_strategy\u001b[49m\u001b[43m=\u001b[49m\u001b[43mvision_feature_select_strategy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1285\u001b[39m \u001b[43m        \u001b[49m\u001b[43mimage_sizes\u001b[49m\u001b[43m=\u001b[49m\u001b[43mimage_sizes\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1286\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1288\u001b[39m     vision_flat = image_features.view(-\u001b[32m1\u001b[39m, image_features.size(-\u001b[32m1\u001b[39m))\n\u001b[32m   1289\u001b[39m     projected_vision_flat = \u001b[38;5;28mself\u001b[39m.multi_modal_projector(vision_flat)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/apps/jupyter/.venv/lib/python3.13/site-packages/transformers/models/llama4/modeling_llama4.py:1200\u001b[39m, in \u001b[36mLlama4ForConditionalGeneration.get_image_features\u001b[39m\u001b[34m(self, pixel_values, vision_feature_layer, vision_feature_select_strategy, **kwargs)\u001b[39m\n\u001b[32m   1198\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mUnexpected select feature strategy: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m.vision_feature_select_strategy\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m   1199\u001b[39m kwargs = {k: v \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m kwargs.items() \u001b[38;5;28;01mif\u001b[39;00m v \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m}\n\u001b[32m-> \u001b[39m\u001b[32m1200\u001b[39m image_outputs = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mvision_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpixel_values\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1201\u001b[39m hidden_state = image_outputs.last_hidden_state\n\u001b[32m   1202\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m hidden_state\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/apps/jupyter/.venv/lib/python3.13/site-packages/torch/nn/modules/module.py:1773\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1771\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1772\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1773\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/apps/jupyter/.venv/lib/python3.13/site-packages/torch/nn/modules/module.py:1784\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1779\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1780\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1781\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1782\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1783\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1784\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1786\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1787\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/apps/jupyter/.venv/lib/python3.13/site-packages/accelerate/hooks.py:175\u001b[39m, in \u001b[36madd_hook_to_module.<locals>.new_forward\u001b[39m\u001b[34m(module, *args, **kwargs)\u001b[39m\n\u001b[32m    173\u001b[39m         output = module._old_forward(*args, **kwargs)\n\u001b[32m    174\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m175\u001b[39m     output = \u001b[43mmodule\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_old_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    176\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m module._hf_hook.post_forward(module, output)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/apps/jupyter/.venv/lib/python3.13/site-packages/transformers/models/llama4/modeling_llama4.py:1082\u001b[39m, in \u001b[36mLlama4VisionModel.forward\u001b[39m\u001b[34m(self, pixel_values, attention_mask, output_attentions, output_hidden_states, return_dict)\u001b[39m\n\u001b[32m   1080\u001b[39m num_concurrent_media = \u001b[32m1\u001b[39m\n\u001b[32m   1081\u001b[39m num_chunks = \u001b[32m1\u001b[39m\n\u001b[32m-> \u001b[39m\u001b[32m1082\u001b[39m hidden_state = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mpatch_embedding\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpixel_values\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1083\u001b[39m _, num_patches, hidden_dim = hidden_state.shape\n\u001b[32m   1085\u001b[39m \u001b[38;5;66;03m# Add cls token\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/apps/jupyter/.venv/lib/python3.13/site-packages/torch/nn/modules/module.py:1773\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1771\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1772\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1773\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/apps/jupyter/.venv/lib/python3.13/site-packages/torch/nn/modules/module.py:1784\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1779\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1780\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1781\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1782\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1783\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1784\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1786\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1787\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/apps/jupyter/.venv/lib/python3.13/site-packages/accelerate/hooks.py:175\u001b[39m, in \u001b[36madd_hook_to_module.<locals>.new_forward\u001b[39m\u001b[34m(module, *args, **kwargs)\u001b[39m\n\u001b[32m    173\u001b[39m         output = module._old_forward(*args, **kwargs)\n\u001b[32m    174\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m175\u001b[39m     output = \u001b[43mmodule\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_old_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    176\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m module._hf_hook.post_forward(module, output)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/apps/jupyter/.venv/lib/python3.13/site-packages/transformers/models/llama4/modeling_llama4.py:979\u001b[39m, in \u001b[36mLlama4UnfoldConvolution.forward\u001b[39m\u001b[34m(self, hidden_states)\u001b[39m\n\u001b[32m    977\u001b[39m hidden_states = \u001b[38;5;28mself\u001b[39m.unfold(hidden_states)\n\u001b[32m    978\u001b[39m hidden_states = hidden_states.permute(\u001b[32m0\u001b[39m, \u001b[32m2\u001b[39m, \u001b[32m1\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m979\u001b[39m hidden_states = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    980\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m hidden_states\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/apps/jupyter/.venv/lib/python3.13/site-packages/torch/nn/modules/module.py:1773\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1771\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1772\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1773\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/apps/jupyter/.venv/lib/python3.13/site-packages/torch/nn/modules/module.py:1784\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1779\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1780\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1781\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1782\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1783\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1784\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1786\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1787\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/apps/jupyter/.venv/lib/python3.13/site-packages/accelerate/hooks.py:175\u001b[39m, in \u001b[36madd_hook_to_module.<locals>.new_forward\u001b[39m\u001b[34m(module, *args, **kwargs)\u001b[39m\n\u001b[32m    173\u001b[39m         output = module._old_forward(*args, **kwargs)\n\u001b[32m    174\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m175\u001b[39m     output = \u001b[43mmodule\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_old_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    176\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m module._hf_hook.post_forward(module, output)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/apps/jupyter/.venv/lib/python3.13/site-packages/torch/nn/modules/linear.py:134\u001b[39m, in \u001b[36mLinear.forward\u001b[39m\u001b[34m(self, input)\u001b[39m\n\u001b[32m    130\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) -> Tensor:\n\u001b[32m    131\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    132\u001b[39m \u001b[33;03m    Runs the forward pass.\u001b[39;00m\n\u001b[32m    133\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m134\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[43m.\u001b[49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[31mRuntimeError\u001b[39m: expected mat1 and mat2 to have the same dtype, but got: c10::BFloat16 != c10::Half"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "import torch\n",
    "\n",
    "pipe = pipeline(\"image-text-to-text\", model=\"meta-llama/Llama-4-Scout-17B-16E\", torch_dtype=torch.float16,\n",
    "    device_map=\"auto\"  # This distributes across available GPUs\n",
    "               )\n",
    "messages = [\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": [\n",
    "            {\"type\": \"image\", \"url\": \"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/p-blog/candy.JPG\"},\n",
    "            {\"type\": \"text\", \"text\": \"What animal is on the candy?\"}\n",
    "        ]\n",
    "    },\n",
    "]\n",
    "pipe(text=messages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3327bf13-b882-4a78-9a4f-a40e1bc96b1d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/apps/jupyter/.venv/lib/python3.13/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Fetching 50 files: 100%|██████████| 50/50 [00:00<00:00, 11703.51it/s]\n",
      "Loading checkpoint shards: 100%|██████████| 50/50 [01:10<00:00,  1.41s/it]\n",
      "Some parameters are on the meta device because they were offloaded to the cpu and disk.\n"
     ]
    }
   ],
   "source": [
    "output_dir = \"/apps/models/huggingface/hub/llama-4-scout-17b-8bit\"\n",
    "\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
    "import torch\n",
    "\n",
    "model_name = \"meta-llama/Llama-4-Scout-17B-16E\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "# Configure 8-bit quantization\n",
    "quantization_config = BitsAndBytesConfig(\n",
    "    load_in_8bit=True,\n",
    "    llm_int8_threshold=6.0,\n",
    "    llm_int8_has_fp16_weight=False,\n",
    "    llm_int8_enable_fp32_cpu_offload=True\n",
    ")\n",
    "\n",
    "# Optimal setup for your hardware\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    torch_dtype=torch.float16,  # Use FP16 to fit in VRAM better\n",
    "    device_map=\"auto\",  # Let it automatically distribute across GPUs\n",
    "#    quantization_config=quantization_config,\n",
    "    low_cpu_mem_usage=True,\n",
    "    max_memory={\n",
    "        0: \"14GB\",      # GPU 0 - leave some headroom\n",
    "        1: \"14GB\",      # GPU 1 - leave some headroom  \n",
    "        \"cpu\": \"100GB\"  # Use most of your RAM for overflow\n",
    "    }\n",
    ")\n",
    "\n",
    "# Save with large shards since you have plenty of RAM\n",
    "#model.save_pretrained(\n",
    "#    output_dir,\n",
    "#    safe_serialization=True,\n",
    "#    max_shard_size=\"16GB\"  # Optimal for your 128GB RAM\n",
    "#)\n",
    "#tokenizer.save_pretrained(output_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1b5f4503-f3e3-4860-8446-554d7b253795",
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "libmlx.so: cannot open shared object file: No such file or directory",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mImportError\u001b[39m                               Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mmlx_lm\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m load, generate\n\u001b[32m      3\u001b[39m model, tokenizer = load(\u001b[33m\"\u001b[39m\u001b[33mmlx-community/meta-llama-Llama-4-Scout-17B-16E-8bit\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m      5\u001b[39m prompt = \u001b[33m\"\u001b[39m\u001b[33mhello\u001b[39m\u001b[33m\"\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/apps/jupyter/.venv/lib/python3.13/site-packages/mlx_lm/__init__.py:9\u001b[39m\n\u001b[32m      5\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_version\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m __version__\n\u001b[32m      7\u001b[39m os.environ[\u001b[33m\"\u001b[39m\u001b[33mTRANSFORMERS_NO_ADVISORY_WARNINGS\u001b[39m\u001b[33m\"\u001b[39m] = \u001b[33m\"\u001b[39m\u001b[33m1\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m9\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mconvert\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m convert\n\u001b[32m     10\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mgenerate\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m generate, stream_generate\n\u001b[32m     11\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mutils\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m load\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/apps/jupyter/.venv/lib/python3.13/site-packages/mlx_lm/convert.py:7\u001b[39m\n\u001b[32m      4\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpathlib\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Path\n\u001b[32m      5\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtyping\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Callable, Optional, Union\n\u001b[32m----> \u001b[39m\u001b[32m7\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mmlx\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mcore\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mmx\u001b[39;00m\n\u001b[32m      8\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mmlx\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mnn\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnn\u001b[39;00m\n\u001b[32m      9\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mmlx\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mutils\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m tree_map_with_path\n",
      "\u001b[31mImportError\u001b[39m: libmlx.so: cannot open shared object file: No such file or directory"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "41716d70-08f0-49c6-8dee-3512bdbe661f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Analyzing model structure...\n",
      "Layer types in model:\n",
      "  Embedding: 1\n",
      "  Llama4ForCausalLM: 1\n",
      "  Llama4TextAttention: 48\n",
      "  Llama4TextDecoderLayer: 48\n",
      "  Llama4TextExperts: 48\n",
      "  Llama4TextL2Norm: 36\n",
      "  Llama4TextMLP: 48\n",
      "  Llama4TextModel: 1\n",
      "  Llama4TextMoe: 48\n",
      "  Llama4TextRMSNorm: 97\n",
      "  Llama4TextRotaryEmbedding: 1\n",
      "  ModuleList: 1\n",
      "  QuantizedLinear: 385\n",
      "  SiLU: 96\n",
      "\n",
      "Found 385 linear-like layers:\n",
      "  model.layers.0.self_attn.q_proj: QuantizedLinear\n",
      "  model.layers.0.self_attn.k_proj: QuantizedLinear\n",
      "  model.layers.0.self_attn.v_proj: QuantizedLinear\n",
      "  model.layers.0.self_attn.o_proj: QuantizedLinear\n",
      "  model.layers.0.feed_forward.router: QuantizedLinear\n",
      "  model.layers.0.feed_forward.shared_expert.gate_proj: QuantizedLinear\n",
      "  model.layers.0.feed_forward.shared_expert.up_proj: QuantizedLinear\n",
      "  model.layers.0.feed_forward.shared_expert.down_proj: QuantizedLinear\n",
      "  model.layers.1.self_attn.q_proj: QuantizedLinear\n",
      "  model.layers.1.self_attn.k_proj: QuantizedLinear\n",
      "  ... and 375 more\n",
      "\n",
      "Attempting to save model without custom quantization...\n",
      "Save failed: `weight_quantized` is not an nn.Parameter\n",
      "\n",
      "Trying manual state dict save...\n",
      "Found 177 meta tensors (will skip these)\n",
      "Saving 17 real parameters\n",
      "Saved chunk 1/1\n",
      "Manual save completed to: /apps/models/huggingface/hub/llama-4-scout-17b-8bit\n",
      "\n",
      "Current GPU Memory usage:\n",
      "GPU 0: 5.85 GB allocated, 6.29 GB reserved\n",
      "GPU 1: 11.78 GB allocated, 12.92 GB reserved\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2e0bbb26-1f60-4944-a542-9478e2bd7d9e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fetching 50 files: 100%|██████████| 50/50 [00:00<00:00, 21358.10it/s]\n",
      "Loading checkpoint shards:   8%|▊         | 4/50 [00:03<00:41,  1.11it/s]\n"
     ]
    },
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 1.25 GiB. GPU 0 has a total capacity of 15.47 GiB of which 688.94 MiB is free. Including non-PyTorch memory, this process has 14.60 GiB memory in use. Of the allocated memory 13.76 GiB is allocated by PyTorch, and 730.15 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mOutOfMemoryError\u001b[39m                          Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 13\u001b[39m\n\u001b[32m      5\u001b[39m quantization_config = BitsAndBytesConfig(\n\u001b[32m      6\u001b[39m     load_in_8bit=\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[32m      7\u001b[39m     llm_int8_threshold=\u001b[32m6.0\u001b[39m,\n\u001b[32m      8\u001b[39m     llm_int8_has_fp16_weight=\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[32m      9\u001b[39m     llm_int8_enable_fp32_cpu_offload=\u001b[38;5;28;01mTrue\u001b[39;00m,  \u001b[38;5;66;03m# This is the key setting\u001b[39;00m\n\u001b[32m     10\u001b[39m )\n\u001b[32m     12\u001b[39m tokenizer = AutoTokenizer.from_pretrained(model_name)\n\u001b[32m---> \u001b[39m\u001b[32m13\u001b[39m model = \u001b[43mAutoModelForCausalLM\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     14\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmodel_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     15\u001b[39m \u001b[43m    \u001b[49m\u001b[43mquantization_config\u001b[49m\u001b[43m=\u001b[49m\u001b[43mquantization_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     16\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdevice_map\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mauto\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     17\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtorch_dtype\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfloat16\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     18\u001b[39m \u001b[43m    \u001b[49m\u001b[43mlow_cpu_mem_usage\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Additional memory optimization\u001b[39;49;00m\n\u001b[32m     19\u001b[39m \u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/apps/jupyter/.venv/lib/python3.13/site-packages/transformers/models/auto/auto_factory.py:600\u001b[39m, in \u001b[36m_BaseAutoModelClass.from_pretrained\u001b[39m\u001b[34m(cls, pretrained_model_name_or_path, *model_args, **kwargs)\u001b[39m\n\u001b[32m    598\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m model_class.config_class == config.sub_configs.get(\u001b[33m\"\u001b[39m\u001b[33mtext_config\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[32m    599\u001b[39m         config = config.get_text_config()\n\u001b[32m--> \u001b[39m\u001b[32m600\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mmodel_class\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    601\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43mmodel_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mhub_kwargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\n\u001b[32m    602\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    603\u001b[39m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m    604\u001b[39m     \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mUnrecognized configuration class \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mconfig.\u001b[34m__class__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m for this kind of AutoModel: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mcls\u001b[39m.\u001b[34m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m    605\u001b[39m     \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mModel type should be one of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[33m'\u001b[39m\u001b[33m, \u001b[39m\u001b[33m'\u001b[39m.join(c.\u001b[34m__name__\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mfor\u001b[39;00m\u001b[38;5;250m \u001b[39mc\u001b[38;5;250m \u001b[39m\u001b[38;5;129;01min\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28mcls\u001b[39m._model_mapping.keys())\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    606\u001b[39m )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/apps/jupyter/.venv/lib/python3.13/site-packages/transformers/modeling_utils.py:315\u001b[39m, in \u001b[36mrestore_default_torch_dtype.<locals>._wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    313\u001b[39m old_dtype = torch.get_default_dtype()\n\u001b[32m    314\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m315\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    316\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m    317\u001b[39m     torch.set_default_dtype(old_dtype)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/apps/jupyter/.venv/lib/python3.13/site-packages/transformers/modeling_utils.py:4998\u001b[39m, in \u001b[36mPreTrainedModel.from_pretrained\u001b[39m\u001b[34m(cls, pretrained_model_name_or_path, config, cache_dir, ignore_mismatched_sizes, force_download, local_files_only, token, revision, use_safetensors, weights_only, *model_args, **kwargs)\u001b[39m\n\u001b[32m   4988\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m dtype_orig \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m   4989\u001b[39m         torch.set_default_dtype(dtype_orig)\n\u001b[32m   4991\u001b[39m     (\n\u001b[32m   4992\u001b[39m         model,\n\u001b[32m   4993\u001b[39m         missing_keys,\n\u001b[32m   4994\u001b[39m         unexpected_keys,\n\u001b[32m   4995\u001b[39m         mismatched_keys,\n\u001b[32m   4996\u001b[39m         offload_index,\n\u001b[32m   4997\u001b[39m         error_msgs,\n\u001b[32m-> \u001b[39m\u001b[32m4998\u001b[39m     ) = \u001b[38;5;28;43mcls\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_load_pretrained_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   4999\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   5000\u001b[39m \u001b[43m        \u001b[49m\u001b[43mstate_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   5001\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcheckpoint_files\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   5002\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   5003\u001b[39m \u001b[43m        \u001b[49m\u001b[43mignore_mismatched_sizes\u001b[49m\u001b[43m=\u001b[49m\u001b[43mignore_mismatched_sizes\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   5004\u001b[39m \u001b[43m        \u001b[49m\u001b[43msharded_metadata\u001b[49m\u001b[43m=\u001b[49m\u001b[43msharded_metadata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   5005\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdevice_map\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdevice_map\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   5006\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdisk_offload_folder\u001b[49m\u001b[43m=\u001b[49m\u001b[43moffload_folder\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   5007\u001b[39m \u001b[43m        \u001b[49m\u001b[43moffload_state_dict\u001b[49m\u001b[43m=\u001b[49m\u001b[43moffload_state_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   5008\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtorch_dtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   5009\u001b[39m \u001b[43m        \u001b[49m\u001b[43mhf_quantizer\u001b[49m\u001b[43m=\u001b[49m\u001b[43mhf_quantizer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   5010\u001b[39m \u001b[43m        \u001b[49m\u001b[43mkeep_in_fp32_regex\u001b[49m\u001b[43m=\u001b[49m\u001b[43mkeep_in_fp32_regex\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   5011\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdevice_mesh\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdevice_mesh\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   5012\u001b[39m \u001b[43m        \u001b[49m\u001b[43mkey_mapping\u001b[49m\u001b[43m=\u001b[49m\u001b[43mkey_mapping\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   5013\u001b[39m \u001b[43m        \u001b[49m\u001b[43mweights_only\u001b[49m\u001b[43m=\u001b[49m\u001b[43mweights_only\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   5014\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   5015\u001b[39m \u001b[38;5;66;03m# make sure token embedding weights are still tied if needed\u001b[39;00m\n\u001b[32m   5016\u001b[39m model.tie_weights()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/apps/jupyter/.venv/lib/python3.13/site-packages/transformers/modeling_utils.py:5456\u001b[39m, in \u001b[36mPreTrainedModel._load_pretrained_model\u001b[39m\u001b[34m(cls, model, state_dict, checkpoint_files, pretrained_model_name_or_path, ignore_mismatched_sizes, sharded_metadata, device_map, disk_offload_folder, offload_state_dict, dtype, hf_quantizer, keep_in_fp32_regex, device_mesh, key_mapping, weights_only)\u001b[39m\n\u001b[32m   5453\u001b[39m         args_list = logging.tqdm(args_list, desc=\u001b[33m\"\u001b[39m\u001b[33mLoading checkpoint shards\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m   5455\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m args \u001b[38;5;129;01min\u001b[39;00m args_list:\n\u001b[32m-> \u001b[39m\u001b[32m5456\u001b[39m         _error_msgs, disk_offload_index, cpu_offload_index = \u001b[43mload_shard_file\u001b[49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   5457\u001b[39m         error_msgs += _error_msgs\n\u001b[32m   5459\u001b[39m \u001b[38;5;66;03m# Adjust offloaded weights name and save if needed\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/apps/jupyter/.venv/lib/python3.13/site-packages/transformers/modeling_utils.py:937\u001b[39m, in \u001b[36mload_shard_file\u001b[39m\u001b[34m(args)\u001b[39m\n\u001b[32m    935\u001b[39m \u001b[38;5;66;03m# Skip it with fsdp on ranks other than 0\u001b[39;00m\n\u001b[32m    936\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (is_fsdp_enabled() \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_local_dist_rank_0() \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_quantized):\n\u001b[32m--> \u001b[39m\u001b[32m937\u001b[39m     disk_offload_index, cpu_offload_index = \u001b[43m_load_state_dict_into_meta_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    938\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmodel_to_load\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    939\u001b[39m \u001b[43m        \u001b[49m\u001b[43mstate_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    940\u001b[39m \u001b[43m        \u001b[49m\u001b[43mshard_file\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    941\u001b[39m \u001b[43m        \u001b[49m\u001b[43mexpected_keys\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    942\u001b[39m \u001b[43m        \u001b[49m\u001b[43mreverse_key_renaming_mapping\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    943\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdevice_map\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdevice_map\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    944\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdisk_offload_folder\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdisk_offload_folder\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    945\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdisk_offload_index\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdisk_offload_index\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    946\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcpu_offload_folder\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcpu_offload_folder\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    947\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcpu_offload_index\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcpu_offload_index\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    948\u001b[39m \u001b[43m        \u001b[49m\u001b[43mhf_quantizer\u001b[49m\u001b[43m=\u001b[49m\u001b[43mhf_quantizer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    949\u001b[39m \u001b[43m        \u001b[49m\u001b[43mis_safetensors\u001b[49m\u001b[43m=\u001b[49m\u001b[43mis_offloaded_safetensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    950\u001b[39m \u001b[43m        \u001b[49m\u001b[43mkeep_in_fp32_regex\u001b[49m\u001b[43m=\u001b[49m\u001b[43mkeep_in_fp32_regex\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    951\u001b[39m \u001b[43m        \u001b[49m\u001b[43munexpected_keys\u001b[49m\u001b[43m=\u001b[49m\u001b[43munexpected_keys\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    952\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdevice_mesh\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdevice_mesh\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    953\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    955\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m error_msgs, disk_offload_index, cpu_offload_index\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/apps/jupyter/.venv/lib/python3.13/site-packages/torch/utils/_contextlib.py:120\u001b[39m, in \u001b[36mcontext_decorator.<locals>.decorate_context\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    117\u001b[39m \u001b[38;5;129m@functools\u001b[39m.wraps(func)\n\u001b[32m    118\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mdecorate_context\u001b[39m(*args, **kwargs):\n\u001b[32m    119\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[32m--> \u001b[39m\u001b[32m120\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/apps/jupyter/.venv/lib/python3.13/site-packages/transformers/modeling_utils.py:849\u001b[39m, in \u001b[36m_load_state_dict_into_meta_model\u001b[39m\u001b[34m(model, state_dict, shard_file, expected_keys, reverse_renaming_mapping, device_map, disk_offload_folder, disk_offload_index, cpu_offload_folder, cpu_offload_index, hf_quantizer, is_safetensors, keep_in_fp32_regex, unexpected_keys, device_mesh)\u001b[39m\n\u001b[32m    846\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m is_fsdp_enabled():\n\u001b[32m    847\u001b[39m         param_device = \u001b[33m\"\u001b[39m\u001b[33mcpu\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m is_local_dist_rank_0() \u001b[38;5;28;01melse\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mmeta\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m--> \u001b[39m\u001b[32m849\u001b[39m     _load_parameter_into_model(model, param_name, \u001b[43mparam\u001b[49m\u001b[43m.\u001b[49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparam_device\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[32m    851\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    852\u001b[39m     hf_quantizer.create_quantized_param(\n\u001b[32m    853\u001b[39m         model, param, param_name, param_device, state_dict, unexpected_keys\n\u001b[32m    854\u001b[39m     )\n",
      "\u001b[31mOutOfMemoryError\u001b[39m: CUDA out of memory. Tried to allocate 1.25 GiB. GPU 0 has a total capacity of 15.47 GiB of which 688.94 MiB is free. Including non-PyTorch memory, this process has 14.60 GiB memory in use. Of the allocated memory 13.76 GiB is allocated by PyTorch, and 730.15 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)"
     ]
    }
   ],
   "source": [
    "#model_name=\"meta-llama/Llama-4-Scout-17B-16E\"\n",
    "model_name='/apps/models/huggingface/hub/llama-4-scout-17b-8bit'\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    quantization_config=quantization_config,\n",
    "    device_map=\"auto\",\n",
    "    torch_dtype=torch.float16,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4327df95-1522-4a1b-8261-de5c683e6dfc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "usage: ipykernel_launcher.py [-h] --model MODEL --outline OUTLINE\n",
      "                             --chapter CHAPTER --number NUMBER\n",
      "                             --instruction INSTRUCTION [--branch]\n",
      "                             [--evaluate EVALUATE]\n",
      "ipykernel_launcher.py: error: the following arguments are required: --model, --outline, --chapter, --number, --instruction\n"
     ]
    },
    {
     "ename": "SystemExit",
     "evalue": "2",
     "output_type": "error",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001b[31mSystemExit\u001b[39m\u001b[31m:\u001b[39m 2\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python3\n",
    "\"\"\"\n",
    "Chapter Processing Script with Transformer Models\n",
    "Processes book chapters using AI models with iterative improvement and evaluation.\n",
    "\"\"\"\n",
    "\n",
    "import argparse\n",
    "import os\n",
    "import sys\n",
    "import subprocess\n",
    "from dataclasses import dataclass, field\n",
    "from typing import List, Optional, Dict, Any\n",
    "import json\n",
    "import re\n",
    "from datetime import datetime\n",
    "\n",
    "try:\n",
    "    from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "    import torch\n",
    "except ImportError:\n",
    "    print(\"Error: transformers and torch are required. Install with:\")\n",
    "    print(\"pip install transformers torch\")\n",
    "    sys.exit(1)\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class Action:\n",
    "    \"\"\"Represents a single edit action on the chapter.\"\"\"\n",
    "    action_type: str  # ADD or DELETE\n",
    "    before_marker: Optional[str] = None\n",
    "    after_marker: Optional[str] = None\n",
    "    content: Optional[str] = None  # Used by ADD\n",
    "    \n",
    "    def __post_init__(self):\n",
    "        if self.action_type not in [\"ADD\", \"DELETE\"]:\n",
    "            raise ValueError(\"action_type must be 'ADD' or 'DELETE'\")\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class ProcessingConfig:\n",
    "    \"\"\"Configuration for chapter processing.\"\"\"\n",
    "    outline_path: str\n",
    "    chapter_path: str\n",
    "    number: int\n",
    "    instruction_path: str\n",
    "    model_name: Optional[str] = None\n",
    "    model: Optional[Any] = None\n",
    "    tokenizer: Optional[Any] = None\n",
    "    evaluate: Optional[str] = None\n",
    "    branch: bool = False\n",
    "    \n",
    "    def __post_init__(self):\n",
    "        if not self.model_name and not self.model:\n",
    "            raise ValueError(\"Either model_name or model must be provided\")\n",
    "        if self.model_name and self.model:\n",
    "            raise ValueError(\"Provide either model_name or model, not both\")\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class Evaluation:\n",
    "    \"\"\"Results from evaluating a chapter iteration.\"\"\"\n",
    "    grade: str\n",
    "    change_quality: str\n",
    "    accept: bool\n",
    "    recommendations: str\n",
    "    \n",
    "    def __post_init__(self):\n",
    "        valid_grades = ['A', 'B', 'C', 'D', 'F']\n",
    "        if self.grade not in valid_grades or self.change_quality not in valid_grades:\n",
    "            raise ValueError(\"Grades must be A, B, C, D, or F\")\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class RuntimeState:\n",
    "    \"\"\"Current state of the chapter processing.\"\"\"\n",
    "    number: int\n",
    "    outline_markdown: str\n",
    "    chapter_markdown: str\n",
    "    original_chapter: str\n",
    "    iteration: int = 0\n",
    "    \n",
    "    def get_chapter_backup(self) -> str:\n",
    "        \"\"\"Get a backup of the current chapter state.\"\"\"\n",
    "        return self.chapter_markdown\n",
    "\n",
    "\n",
    "class ChapterProcessor:\n",
    "    \"\"\"Main class for processing chapters with transformer models.\"\"\"\n",
    "    \n",
    "    def __init__(self, config: ProcessingConfig):\n",
    "        \"\"\"Initialize the processor with a configuration object.\"\"\"\n",
    "        self.config = config\n",
    "        self.model_name = config.model_name\n",
    "        self.tokenizer = config.tokenizer\n",
    "        self.model = config.model\n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        \n",
    "        # If model is provided, we assume it's already loaded\n",
    "        self.model_loaded = config.model is not None and config.tokenizer is not None\n",
    "        \n",
    "        # Default format instructions\n",
    "        self.action_format = \"\"\"\n",
    "Output your response as a JSON array of actions. Each action should have:\n",
    "{\n",
    "  \"action_type\": \"ADD\" or \"DELETE\",\n",
    "  \"before_marker\": \"text that comes before the change (optional)\",\n",
    "  \"after_marker\": \"text that comes after the change (optional)\", \n",
    "  \"content\": \"content to add (for ADD actions only)\"\n",
    "}\n",
    "\n",
    "Example:\n",
    "[\n",
    "  {\n",
    "    \"action_type\": \"ADD\",\n",
    "    \"before_marker\": \"## Introduction\",\n",
    "    \"after_marker\": \"The main concept\",\n",
    "    \"content\": \"This chapter explores the fundamental principles that will guide our understanding.\"\n",
    "  },\n",
    "  {\n",
    "    \"action_type\": \"DELETE\",\n",
    "    \"before_marker\": \"outdated information\",\n",
    "    \"after_marker\": \"continues with\"\n",
    "  }\n",
    "]\n",
    "\"\"\"\n",
    "        \n",
    "        self.evaluation_format = \"\"\"\n",
    "Output your evaluation as JSON:\n",
    "{\n",
    "  \"grade\": \"A-F letter grade for overall chapter quality\",\n",
    "  \"change_quality\": \"A-F letter grade for quality of changes made\", \n",
    "  \"accept\": true/false whether changes should be accepted,\n",
    "  \"recommendations\": \"5-7 sentences about what still needs improvement\"\n",
    "}\n",
    "\"\"\"\n",
    "        \n",
    "        self.default_evaluation = \"\"\"\n",
    "Evaluate this chapter for:\n",
    "1. Clarity and readability\n",
    "2. Logical flow and organization\n",
    "3. Completeness relative to outline\n",
    "4. Quality of recent changes\n",
    "5. Overall coherence with the book structure\n",
    "\"\"\"\n",
    "    \n",
    "    def load_model(self):\n",
    "        \"\"\"Load the transformer model and tokenizer.\"\"\"\n",
    "        if self.model_loaded:\n",
    "            print(\"Using pre-loaded model\")\n",
    "            return\n",
    "            \n",
    "        if not self.model_name:\n",
    "            raise ValueError(\"Either model_name must be provided or model must be pre-loaded\")\n",
    "            \n",
    "        print(f\"Loading model: {self.model_name}\")\n",
    "        try:\n",
    "            self.tokenizer = AutoTokenizer.from_pretrained(self.model_name)\n",
    "            self.model = AutoModelForCausalLM.from_pretrained(\n",
    "                self.model_name,\n",
    "                torch_dtype=torch.float16 if self.device.type == \"cuda\" else torch.float32,\n",
    "                device_map=\"auto\" if self.device.type == \"cuda\" else None\n",
    "            )\n",
    "            \n",
    "            # Add pad token if it doesn't exist\n",
    "            if self.tokenizer.pad_token is None:\n",
    "                self.tokenizer.pad_token = self.tokenizer.eos_token\n",
    "                \n",
    "            self.model_loaded = True\n",
    "            print(f\"Model loaded successfully on {self.device}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading model: {e}\")\n",
    "            sys.exit(1)\n",
    "    \n",
    "    def read_file(self, filepath: str) -> str:\n",
    "        \"\"\"Read content from a file.\"\"\"\n",
    "        try:\n",
    "            with open(filepath, 'r', encoding='utf-8') as f:\n",
    "                return f.read()\n",
    "        except FileNotFoundError:\n",
    "            print(f\"Error: File not found: {filepath}\")\n",
    "            sys.exit(1)\n",
    "        except Exception as e:\n",
    "            print(f\"Error reading file {filepath}: {e}\")\n",
    "            sys.exit(1)\n",
    "    \n",
    "    def create_git_branch(self, chapter_number: int):\n",
    "        \"\"\"Create a new git branch for this processing session.\"\"\"\n",
    "        branch_name = f\"chapter-{chapter_number}-processing-{datetime.now().strftime('%Y%m%d-%H%M%S')}\"\n",
    "        try:\n",
    "            subprocess.run([\"git\", \"checkout\", \"-b\", branch_name], check=True, capture_output=True)\n",
    "            print(f\"Created git branch: {branch_name}\")\n",
    "            return branch_name\n",
    "        except subprocess.CalledProcessError as e:\n",
    "            print(f\"Warning: Could not create git branch: {e}\")\n",
    "            return None\n",
    "    \n",
    "    def generate_response(self, prompt: str, max_length: int = 2048) -> str:\n",
    "        \"\"\"Generate a response from the model.\"\"\"\n",
    "        inputs = self.tokenizer.encode(prompt, return_tensors=\"pt\", truncation=True, max_length=2048)\n",
    "        inputs = inputs.to(self.device)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            outputs = self.model.generate(\n",
    "                inputs,\n",
    "                max_length=len(inputs[0]) + max_length,\n",
    "                num_return_sequences=1,\n",
    "                temperature=0.7,\n",
    "                do_sample=True,\n",
    "                pad_token_id=self.tokenizer.pad_token_id,\n",
    "                eos_token_id=self.tokenizer.eos_token_id\n",
    "            )\n",
    "        \n",
    "        # Decode only the generated part\n",
    "        generated_text = self.tokenizer.decode(outputs[0][len(inputs[0]):], skip_special_tokens=True)\n",
    "        return generated_text.strip()\n",
    "    \n",
    "    def process(self, state: RuntimeState, instruction: str) -> List[Action]:\n",
    "        \"\"\"Process the chapter and return a list of actions.\"\"\"\n",
    "        prompt = f\"\"\"Task: {instruction}\n",
    "\n",
    "Outline: <outline>{state.outline_markdown}</outline>\n",
    "\n",
    "Chapter: <chapter>{state.chapter_markdown}</chapter>\n",
    "\n",
    "{self.action_format}\n",
    "\n",
    "Actions:\"\"\"\n",
    "        \n",
    "        response = self.generate_response(prompt)\n",
    "        \n",
    "        # Extract JSON from response\n",
    "        try:\n",
    "            # Look for JSON array in the response\n",
    "            json_match = re.search(r'\\[.*?\\]', response, re.DOTALL)\n",
    "            if json_match:\n",
    "                actions_data = json.loads(json_match.group())\n",
    "            else:\n",
    "                # Try to parse the entire response as JSON\n",
    "                actions_data = json.loads(response)\n",
    "            \n",
    "            actions = []\n",
    "            for action_data in actions_data:\n",
    "                action = Action(\n",
    "                    action_type=action_data[\"action_type\"],\n",
    "                    before_marker=action_data.get(\"before_marker\"),\n",
    "                    after_marker=action_data.get(\"after_marker\"),\n",
    "                    content=action_data.get(\"content\")\n",
    "                )\n",
    "                actions.append(action)\n",
    "            \n",
    "            return actions\n",
    "        except (json.JSONDecodeError, KeyError) as e:\n",
    "            print(f\"Error parsing actions from model response: {e}\")\n",
    "            print(f\"Model response: {response}\")\n",
    "            return []\n",
    "    \n",
    "    def apply_actions(self, state: RuntimeState, actions: List[Action]) -> bool:\n",
    "        \"\"\"Apply actions to the chapter markdown.\"\"\"\n",
    "        if not actions:\n",
    "            return False\n",
    "        \n",
    "        modified = False\n",
    "        chapter_text = state.chapter_markdown\n",
    "        \n",
    "        for action in actions:\n",
    "            if action.action_type == \"ADD\":\n",
    "                if action.before_marker and action.after_marker:\n",
    "                    # Insert between markers\n",
    "                    pattern = f\"({re.escape(action.before_marker)})(.*?)({re.escape(action.after_marker)})\"\n",
    "                    replacement = f\"\\\\1\\\\2{action.content}\\\\3\"\n",
    "                    new_text = re.sub(pattern, replacement, chapter_text, flags=re.DOTALL)\n",
    "                    if new_text != chapter_text:\n",
    "                        chapter_text = new_text\n",
    "                        modified = True\n",
    "                elif action.before_marker:\n",
    "                    # Insert after marker\n",
    "                    pattern = f\"({re.escape(action.before_marker)})\"\n",
    "                    replacement = f\"\\\\1{action.content}\"\n",
    "                    new_text = re.sub(pattern, replacement, chapter_text)\n",
    "                    if new_text != chapter_text:\n",
    "                        chapter_text = new_text\n",
    "                        modified = True\n",
    "                else:\n",
    "                    # Append to end\n",
    "                    chapter_text += f\"\\n{action.content}\"\n",
    "                    modified = True\n",
    "                    \n",
    "            elif action.action_type == \"DELETE\":\n",
    "                if action.before_marker and action.after_marker:\n",
    "                    # Delete between markers\n",
    "                    pattern = f\"{re.escape(action.before_marker)}(.*?){re.escape(action.after_marker)}\"\n",
    "                    new_text = re.sub(pattern, f\"{action.before_marker}{action.after_marker}\", chapter_text, flags=re.DOTALL)\n",
    "                    if new_text != chapter_text:\n",
    "                        chapter_text = new_text\n",
    "                        modified = True\n",
    "        \n",
    "        if modified:\n",
    "            state.chapter_markdown = chapter_text\n",
    "        \n",
    "        return modified\n",
    "    \n",
    "    def evaluate_iteration(self, state: RuntimeState, evaluation_instruction: str) -> Evaluation:\n",
    "        \"\"\"Evaluate the current iteration of the chapter.\"\"\"\n",
    "        prompt = f\"\"\"Evaluate: {evaluation_instruction}\n",
    "\n",
    "Original Chapter: <original>{state.original_chapter}</original>\n",
    "\n",
    "Current Chapter: <current>{state.chapter_markdown}</current>\n",
    "\n",
    "Outline: <outline>{state.outline_markdown}</outline>\n",
    "\n",
    "{self.evaluation_format}\n",
    "\n",
    "Evaluation:\"\"\"\n",
    "        \n",
    "        response = self.generate_response(prompt)\n",
    "        \n",
    "        try:\n",
    "            # Extract JSON from response\n",
    "            json_match = re.search(r'\\{.*?\\}', response, re.DOTALL)\n",
    "            if json_match:\n",
    "                eval_data = json.loads(json_match.group())\n",
    "            else:\n",
    "                eval_data = json.loads(response)\n",
    "            \n",
    "            return Evaluation(\n",
    "                grade=eval_data[\"grade\"],\n",
    "                change_quality=eval_data[\"change_quality\"],\n",
    "                accept=eval_data[\"accept\"],\n",
    "                recommendations=eval_data[\"recommendations\"]\n",
    "            )\n",
    "        except (json.JSONDecodeError, KeyError) as e:\n",
    "            print(f\"Error parsing evaluation: {e}\")\n",
    "            print(f\"Model response: {response}\")\n",
    "            # Return a default failing evaluation\n",
    "            return Evaluation(\n",
    "                grade=\"F\",\n",
    "                change_quality=\"F\", \n",
    "                accept=False,\n",
    "                recommendations=\"Could not evaluate due to parsing error.\"\n",
    "            )\n",
    "    \n",
    "    def commit_changes(self, state: RuntimeState, evaluation: Evaluation):\n",
    "        \"\"\"Commit changes to git if they're good enough.\"\"\"\n",
    "        chapter_file = f\"Chapters/{state.number}_*.md\"  # Will need actual filename\n",
    "        \n",
    "        try:\n",
    "            # Write the updated chapter\n",
    "            chapter_files = [f for f in os.listdir(\"Chapters\") if f.startswith(f\"{state.number}_\")]\n",
    "            if chapter_files:\n",
    "                chapter_path = os.path.join(\"Chapters\", chapter_files[0])\n",
    "                with open(chapter_path, 'w', encoding='utf-8') as f:\n",
    "                    f.write(state.chapter_markdown)\n",
    "                \n",
    "                # Git add and commit\n",
    "                subprocess.run([\"git\", \"add\", chapter_path], check=True)\n",
    "                commit_msg = f\"Chapter {state.number} iteration {state.iteration}: Grade {evaluation.grade}, Change Quality {evaluation.change_quality}\"\n",
    "                subprocess.run([\"git\", \"commit\", \"-m\", commit_msg], check=True)\n",
    "                print(f\"Committed changes: {commit_msg}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Warning: Could not commit changes: {e}\")\n",
    "    \n",
    "    def process_chapter(self, *, instructions: Optional[str] = None, evaluation: Optional[str] = None) -> bool:\n",
    "        \"\"\"Main processing loop for a chapter.\"\"\"\n",
    "        # Load model\n",
    "        self.load_model()\n",
    "        \n",
    "        # Read files\n",
    "        outline = self.read_file(self.config.outline_path)\n",
    "        chapter = self.read_file(self.config.chapter_path)\n",
    "        \n",
    "        # Use provided instructions or read from file\n",
    "        if instructions is not None:\n",
    "            instruction = instructions\n",
    "        else:\n",
    "            instruction = self.read_file(self.config.instruction_path)\n",
    "        \n",
    "        # Create initial state\n",
    "        state = RuntimeState(\n",
    "            number=self.config.number,\n",
    "            outline_markdown=outline,\n",
    "            chapter_markdown=chapter,\n",
    "            original_chapter=chapter\n",
    "        )\n",
    "        \n",
    "        # Create git branch if requested\n",
    "        branch = None\n",
    "        if self.config.branch:\n",
    "            branch = self.create_git_branch(self.config.number)\n",
    "        \n",
    "        # Determine evaluation instruction\n",
    "        if evaluation is not None:\n",
    "            eval_instruction = evaluation\n",
    "        elif self.config.evaluate is not None:\n",
    "            eval_instruction = self.config.evaluate\n",
    "        else:\n",
    "            eval_instruction = self.default_evaluation\n",
    "        \n",
    "        max_iterations = 5  # Prevent infinite loops\n",
    "        best_grade = None  # Start with no grade achieved\n",
    "        grade_values = {'A': 5, 'B': 4, 'C': 3, 'D': 2, 'F': 1}\n",
    "        \n",
    "        print(f\"\\nStarting processing of Chapter {self.config.number}\")\n",
    "        print(f\"Max iterations: {max_iterations}\")\n",
    "        print(\"-\" * 50)\n",
    "        \n",
    "        for iteration in range(max_iterations):\n",
    "            state.iteration = iteration + 1\n",
    "            print(f\"\\nIteration {state.iteration}:\")\n",
    "            \n",
    "            # Process: Get actions from model\n",
    "            print(\"  Generating actions...\")\n",
    "            actions = self.process(state, instruction)\n",
    "            \n",
    "            if not actions:\n",
    "                print(\"  No actions generated, stopping.\")\n",
    "                break\n",
    "            \n",
    "            print(f\"  Generated {len(actions)} actions\")\n",
    "            \n",
    "            # Apply actions\n",
    "            print(\"  Applying actions...\")\n",
    "            modified = self.apply_actions(state, actions)\n",
    "            \n",
    "            if not modified:\n",
    "                print(\"  No changes made, stopping.\")\n",
    "                break\n",
    "            \n",
    "            # Evaluate\n",
    "            print(\"  Evaluating changes...\")\n",
    "            eval_result = self.evaluate_iteration(state, eval_instruction)\n",
    "            \n",
    "            print(f\"  Grade: {eval_result.grade}\")\n",
    "            print(f\"  Change Quality: {eval_result.change_quality}\")\n",
    "            print(f\"  Accept: {eval_result.accept}\")\n",
    "            print(f\"  Recommendations: {eval_result.recommendations}\")\n",
    "            \n",
    "            # Commit if accepted\n",
    "            if eval_result.accept:\n",
    "                self.commit_changes(state, eval_result)\n",
    "                # Update best grade if this is better (or first grade)\n",
    "                if best_grade is None or grade_values[eval_result.grade] > grade_values[best_grade]:\n",
    "                    best_grade = eval_result.grade\n",
    "                \n",
    "                # Stop if we got an A\n",
    "                if eval_result.grade == 'A':\n",
    "                    print(\"  Achieved grade A, stopping.\")\n",
    "                    break\n",
    "            else:\n",
    "                # Revert changes\n",
    "                print(\"  Changes rejected, reverting...\")\n",
    "                state.chapter_markdown = state.get_chapter_backup()\n",
    "        \n",
    "        print(f\"\\nProcessing complete. Best grade achieved: {best_grade or 'None'}\")\n",
    "        return best_grade is not None and best_grade in ['A', 'B', 'C']  # Success means C or better\n",
    "\n",
    "\n",
    "def main():\n",
    "    \"\"\"Main entry point for command line usage.\"\"\"\n",
    "    parser = argparse.ArgumentParser(description=\"Process book chapters with transformer models\")\n",
    "    parser.add_argument(\"--model\", required=True, help=\"Model name or path\")\n",
    "    parser.add_argument(\"--outline\", required=True, help=\"Path to outline.md\")\n",
    "    parser.add_argument(\"--chapter\", required=True, help=\"Path to chapter markdown file\")\n",
    "    parser.add_argument(\"--number\", type=int, required=True, help=\"Chapter number\")\n",
    "    parser.add_argument(\"--instruction\", required=True, help=\"Path to instructions.md\")\n",
    "    parser.add_argument(\"--branch\", action=\"store_true\", help=\"Create a git branch for this processing session\")\n",
    "    parser.add_argument(\"--evaluate\", help=\"Custom evaluation instructions\")\n",
    "    \n",
    "    args = parser.parse_args()\n",
    "    \n",
    "    # Create config from args\n",
    "    config = ProcessingConfig(\n",
    "        outline_path=args.outline,\n",
    "        chapter_path=args.chapter,\n",
    "        number=args.number,\n",
    "        instruction_path=args.instruction,\n",
    "        model_name=args.model,\n",
    "        evaluate=args.evaluate,\n",
    "        branch=args.branch\n",
    "    )\n",
    "    \n",
    "    # Validate files exist\n",
    "    for filepath in [config.outline_path, config.chapter_path, config.instruction_path]:\n",
    "        if not os.path.exists(filepath):\n",
    "            print(f\"Error: File does not exist: {filepath}\")\n",
    "            sys.exit(1)\n",
    "    \n",
    "    # Initialize and run processor\n",
    "    processor = ChapterProcessor(config)\n",
    "    success = processor.process_chapter()\n",
    "    \n",
    "    sys.exit(0 if success else 1)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cd0a7ba6-3bda-4d41-98d1-46dfe6c35472",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[7]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m config = ProcessingConfig(\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m         model=\u001b[43mmodel\u001b[49m,\n\u001b[32m      3\u001b[39m         outline_path=\u001b[33m'\u001b[39m\u001b[33m/apps/git/book-Fuzz_Testing/README.md\u001b[39m\u001b[33m'\u001b[39m,\n\u001b[32m      4\u001b[39m         chapter_path=\u001b[33m'\u001b[39m\u001b[33m/apps/git/book-Fuzz_Testing/Chapters/1_Introduction.md\u001b[39m\u001b[33m'\u001b[39m,\n\u001b[32m      5\u001b[39m         number=\u001b[32m1\u001b[39m\n\u001b[32m      6\u001b[39m     )\n",
      "\u001b[31mNameError\u001b[39m: name 'model' is not defined"
     ]
    }
   ],
   "source": [
    "config = ProcessingConfig(\n",
    "        model=model,\n",
    "        outline_path='/apps/git/book-Fuzz_Testing/README.md',\n",
    "        chapter_path='/apps/git/book-Fuzz_Testing/Chapters/1_Introduction.md',\n",
    "        number=1\n",
    "    )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
